{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":8757105,"sourceType":"datasetVersion","datasetId":5260894},{"sourceId":185154041,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport utilities\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import VotingClassifier\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-26T10:41:39.389652Z","iopub.execute_input":"2024-06-26T10:41:39.390046Z","iopub.status.idle":"2024-06-26T10:41:43.240737Z","shell.execute_reply.started":"2024-06-26T10:41:39.390002Z","shell.execute_reply":"2024-06-26T10:41:43.239712Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# We import glove file in order to explore new method based on its word vect representation","metadata":{}},{"cell_type":"code","source":"\"\"\"embed_dict = {}\nword_dict = {}\nwith open(\"/kaggle/input/glove-100d/glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        word_dict[word] = word\n        vector = np.asarray(values[1:], \"float32\")\n        embed_dict[word] = vector\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:43.242956Z","iopub.execute_input":"2024-06-26T10:41:43.243667Z","iopub.status.idle":"2024-06-26T10:41:43.251751Z","shell.execute_reply.started":"2024-06-26T10:41:43.243624Z","shell.execute_reply":"2024-06-26T10:41:43.250689Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'embed_dict = {}\\nword_dict = {}\\nwith open(\"/kaggle/input/glove-100d/glove.6B.100d.txt\", \\'r\\', encoding=\"utf-8\") as f:\\n    for line in f:\\n        values = line.split()\\n        word = values[0]\\n        word_dict[word] = word\\n        vector = np.asarray(values[1:], \"float32\")\\n        embed_dict[word] = vector'"},"metadata":{}}]},{"cell_type":"code","source":"def clean_data(text):\n    \"\"\" The goal og this function is to clean a text by removing stopwords, special caracters...\n\n    Args:\n        text (str): \n            text we want to clean\n        \n        acronymes (dict): \n            dict of accronymes with its long word associated\n\n    Returns:\n        text : \n            the text cleaned\n    \"\"\"\n\n    text = str(text)\n    #-------------------------------------------------------------------------------------------------\n\n    ## on gère tout ce qui est distances, mesures... (ex : 12inchs, 12cm, 12gm...)\n    ## en gros on supprime tous les mots qui ont des nombres collés\n    \n    text = re.sub(r'\\d+', '', text) ## on supp les chiffres collés aux mots\n    \n    #-------------------------------------------------------------------------------------------------\n    ## on met tout en minuscule\n    text = text.lower()\n    \n    #-------------------------------------------------------------------------------------------------\n\n    ## on retire la ponctuation\n    a_supp = r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n    text = re.sub(rf'[{re.escape(a_supp)}]', '', text)\n    \n    #-------------------------------------------------------------------------------------------------\n\n    ## on retire les liens\n    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n    text = url.sub(r'', text)\n    \n    html = re.compile(r\"<.*?>\")\n    text = html.sub(r'', text)\n    \n    #-------------------------------------------------------------------------------------------------\n    ## on retire les émojis\n    \n    emoji = re.compile(r\"[\\U0001F600-\\U0001F64F\"  # Emoticons\n                   \"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n                   \"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n                   \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n                   \"\\U00002702-\\U000027B0\"  # Dingbats\n                   \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n                   \"]+\", \n                  flags=re.UNICODE)\n    \n    text = emoji.sub(r'', text)\n\n        \n    #-------------------------------------------------------------------------------------------------    # on retire les mots communs\n    \n    STOPWORDS = set(stopwords.words('english'))\n\n    text = \" \".join([word for word in text.split() if word not in STOPWORDS])\n\n    #-------------------------------------------------------------------------------------------------\n    \n    return text\n\ndef glove_mean(text):\n    \"\"\"\n    Count mean vect of text using glove word vect representation\n    \n    Args :\n        text (str) :\n            string we want to vectorize\n            \n    Returns :\n        the mean vect for the text using glove vect representation\n    \"\"\"\n    mean_vect = []\n    for i in text.split():\n        if i in embed_dict.keys():\n            mean_vect.append(embed_dict[i])\n    return np.mean(mean_vect, axis= 0)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:43.253259Z","iopub.execute_input":"2024-06-26T10:41:43.253608Z","iopub.status.idle":"2024-06-26T10:41:43.265843Z","shell.execute_reply.started":"2024-06-26T10:41:43.253580Z","shell.execute_reply":"2024-06-26T10:41:43.264810Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:43.268315Z","iopub.execute_input":"2024-06-26T10:41:43.269342Z","iopub.status.idle":"2024-06-26T10:41:43.387501Z","shell.execute_reply.started":"2024-06-26T10:41:43.269303Z","shell.execute_reply":"2024-06-26T10:41:43.386498Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_test = test[\"text\"].apply(clean_data).tolist()\nid_test = test[\"id\"]\n\nX_train = train[\"text\"].apply(clean_data).tolist()\nY_train = train[\"target\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:43.388888Z","iopub.execute_input":"2024-06-26T10:41:43.389210Z","iopub.status.idle":"2024-06-26T10:41:45.172162Z","shell.execute_reply.started":"2024-06-26T10:41:43.389182Z","shell.execute_reply":"2024-06-26T10:41:45.171156Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# TFIDF","metadata":{}},{"cell_type":"markdown","source":"TF-IDF (Term Frequency-Inverse Document Frequency): this is a widely-used method that calculates a score for each term by combining a value that depends on its frequency of appearance in a text (term frequency) and a second value that depends on its appearance in all texts. The advantages are that the method is simple to implement and therefore effective, filters out common terms by lowering their scores, and highlights important rare terms. However, sensitivity to rare terms can exaggerate the score, semantic relations are not taken into account and the method loses effectiveness on long texts.","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer() \n\ntfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test_vectors = tfidf_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:45.173272Z","iopub.execute_input":"2024-06-26T10:41:45.173589Z","iopub.status.idle":"2024-06-26T10:41:45.381180Z","shell.execute_reply.started":"2024-06-26T10:41:45.173562Z","shell.execute_reply":"2024-06-26T10:41:45.379915Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(tfidf_train_vectors,\n                                                  Y_train,\n                                                  test_size = 0.3, random_state = 42)\nprint(x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:45.382415Z","iopub.execute_input":"2024-06-26T10:41:45.382755Z","iopub.status.idle":"2024-06-26T10:41:45.393786Z","shell.execute_reply.started":"2024-06-26T10:41:45.382727Z","shell.execute_reply":"2024-06-26T10:41:45.392515Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(5329, 21445)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nXGB = XGBClassifier()\n    \n\nXGB.fit(x_train, y_train)\n\npredictions_xgb = XGB.predict(x_val)\n\nprint(\"Accuracy : \",  accuracy_score(y_val, predictions_xgb))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:45.395162Z","iopub.execute_input":"2024-06-26T10:41:45.395504Z","iopub.status.idle":"2024-06-26T10:41:47.655332Z","shell.execute_reply.started":"2024-06-26T10:41:45.395456Z","shell.execute_reply":"2024-06-26T10:41:47.654362Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Accuracy :  0.7666374781085814\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Bagging","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\ndef bag_model(model, x_train, y_train, x_val, y_val):\n    bag = BaggingClassifier(estimator=model,\n                            n_estimators=10, \n                            random_state=0).fit(x_train, y_train)\n\n    predictions_bag = bag.predict(x_val)\n\n    print(\"accuracy bag: \",  accuracy_score(y_val, predictions_bag))\n\n    # Utiliser la validation croisée pour évaluer la performance du modèle\n    scores = cross_val_score(bag, x_train, y_train, cv=5, scoring='accuracy')\n    print(\"Cross-validation accuracy scores: \", scores)\n    print(\"Mean cross-validation accuracy: \", scores.mean())\n    return bag","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:47.656910Z","iopub.execute_input":"2024-06-26T10:41:47.657529Z","iopub.status.idle":"2024-06-26T10:41:47.664188Z","shell.execute_reply.started":"2024-06-26T10:41:47.657470Z","shell.execute_reply":"2024-06-26T10:41:47.663030Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"bag_xgb = bag_model(XGBClassifier(random_state=42), x_train, y_train, x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:47.667839Z","iopub.execute_input":"2024-06-26T10:41:47.668617Z","iopub.status.idle":"2024-06-26T10:43:24.768779Z","shell.execute_reply.started":"2024-06-26T10:41:47.668584Z","shell.execute_reply":"2024-06-26T10:43:24.767969Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"accuracy bag:  0.7670753064798599\nCross-validation accuracy scores:  [0.76641651 0.7804878  0.76172608 0.75984991 0.76901408]\nMean cross-validation accuracy:  0.7674988769389318\n","output_type":"stream"}]},{"cell_type":"code","source":"bag_cat = bag_model(CatBoostClassifier(random_state=42, verbose=0), x_train, y_train, x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:43:24.769871Z","iopub.execute_input":"2024-06-26T10:43:24.770830Z","iopub.status.idle":"2024-06-26T11:25:49.688907Z","shell.execute_reply.started":"2024-06-26T10:43:24.770800Z","shell.execute_reply":"2024-06-26T11:25:49.687586Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"accuracy bag:  0.7727670753064798\nCross-validation accuracy scores:  [0.7673546  0.7879925  0.75984991 0.76547842 0.76338028]\nMean cross-validation accuracy:  0.7688111407657956\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Voting","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(random_state=42)\ncat = CatBoostClassifier(random_state=42, verbose=0)\nlgb = LGBMClassifier(random_state = 42, verbose=-1)\n\ndef voting_models(models, x_train, y_train, x_val, y_val):\n    \n    voting = VotingClassifier(estimators=models,\n                                    voting='soft')\n\n    voting.fit(x_train, y_train)\n\n    predictions_boost = voting.predict(x_val)\n\n    print(\"accuracy boost: \",  accuracy_score(y_val, predictions_boost))\n\n    # Utiliser la validation croisée pour évaluer la performance du modèle\n    scores = cross_val_score(voting, x_train, y_train, cv=5, scoring='accuracy')\n    print(\"Cross-validation accuracy scores: \", scores)\n    print(\"Mean cross-validation accuracy: \", scores.mean())\n    \n    return voting\n\nmodels = [('xgb', xgb), ('cat', cat), ('lgb', lgb)]\n\nvot = voting_models(models, x_train, y_train, x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:25:49.690302Z","iopub.execute_input":"2024-06-26T11:25:49.691413Z","iopub.status.idle":"2024-06-26T11:30:21.613050Z","shell.execute_reply.started":"2024-06-26T11:25:49.691372Z","shell.execute_reply":"2024-06-26T11:30:21.611878Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"accuracy boost:  0.7732049036777583\nCross-validation accuracy scores:  [0.77204503 0.78705441 0.76454034 0.76078799 0.76525822]\nMean cross-validation accuracy:  0.7699371966634077\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"def submission(model, test, Id):\n    pred = model.predict(test)\n\n    submission = pd.DataFrame({'id': Id,\n                               'target': pred})\n\n    # Save submission to a CSV file\n    submission.to_csv('submission.csv', index=False)\n    \nsubmission(vot, tfidf_test_vectors, id_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:30:21.614271Z","iopub.execute_input":"2024-06-26T11:30:21.614600Z","iopub.status.idle":"2024-06-26T11:30:21.748773Z","shell.execute_reply.started":"2024-06-26T11:30:21.614567Z","shell.execute_reply":"2024-06-26T11:30:21.747527Z"},"trusted":true},"execution_count":16,"outputs":[]}]}