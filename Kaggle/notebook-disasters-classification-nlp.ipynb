{"cells":[{"cell_type":"code","execution_count":118,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-30T19:10:27.969879Z","iopub.status.busy":"2024-06-30T19:10:27.968805Z","iopub.status.idle":"2024-06-30T19:10:28.002121Z","shell.execute_reply":"2024-06-30T19:10:28.001025Z","shell.execute_reply.started":"2024-06-30T19:10:27.969838Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/avicenne/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/avicenne/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","#import utilities\n","\n","# text processing\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","from nltk.stem import WordNetLemmatizer \n","\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# models\n","from catboost import CatBoostClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.ensemble import VotingClassifier\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:10:54.114899Z","iopub.status.busy":"2024-06-30T19:10:54.114542Z","iopub.status.idle":"2024-06-30T19:11:23.487460Z","shell.execute_reply":"2024-06-30T19:11:23.486028Z","shell.execute_reply.started":"2024-06-30T19:10:54.114869Z"},"trusted":true},"outputs":[],"source":["#!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n"]},{"cell_type":"markdown","metadata":{},"source":["# Function to clean the text"]},{"cell_type":"code","execution_count":218,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:11:25.772971Z","iopub.status.busy":"2024-06-30T19:11:25.772514Z","iopub.status.idle":"2024-06-30T19:11:25.785973Z","shell.execute_reply":"2024-06-30T19:11:25.784737Z","shell.execute_reply.started":"2024-06-30T19:11:25.772919Z"},"trusted":true},"outputs":[],"source":["def remove_stop_word(text):\n","    \"\"\"\n","        remove stop word in text, \n","        such as the, as, is...\n","        \n","        Args : \n","            text (str) :\n","                the text we want to remove stopword\n","                \n","        returns :\n","            text without stop word\n","    \"\"\"\n","    \n","    STOPWORDS = set(stopwords.words('english'))\n","\n","    return \" \".join([word for word in text.split() if word not in STOPWORDS])\n","    \n","\n","def del_pattern(pattern, text):\n","    \"\"\"\n","        Del specific pattern in text\n","        \n","        Args :\n","            pattern (str) : \n","                the pattern we want to remove\n","            \n","            text (str) :\n","                the text we want to clean\n","        \n","        Returns :\n","            text without pattern\n","    \"\"\"\n","    \n","    return re.sub(pattern, '', text)\n","\n","def lemmatize(text):\n","    \"\"\"\n","        Lemmatize text : \n","        bats -> bat\n","        cats -> cat\n","        \n","        Args :\n","            text (str) :\n","                the text we want to lemmatize\n","                \n","        returns :\n","            the text lemmatize\n","    \"\"\"\n","    \n","    lemmatizer = WordNetLemmatizer()\n","    return ' '.join([lemmatizer.lemmatize(w) for w in text.split()])\n","\n","\n","def del_patterns(patterns, text):\n","    \"\"\"\n","        Del specific patterns in text\n","        \n","        Args :\n","            pattern (list) : \n","                the patterns we want to remove\n","            \n","            text (str) :\n","                the text we want to clean\n","        \n","        Returns :\n","            text without patterns\n","    \"\"\"\n","    \n","    for pattern in patterns:\n","        text = del_pattern(pattern, text)\n","    \n","    return text\n","\n","def clean_text(text, correction_dict):\n","    \"\"\" \n","        The goal og this function is to clean a text by removing stopwords, \n","        special caracters...\n","\n","        Args:\n","            text (str): \n","                text we want to clean\n","                \n","            correction_dict (dict):\n","                word associated to its correction\n","                mainly disaster words\n","\n","        Returns:\n","            the text cleaned\n","    \"\"\"\n","    \n","    text = str(text).lower()\n","    \n","    special_char = r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n","    html = re.compile(r\"<.*?>\")    \n","    num_in_words = r'\\b\\w*\\d+\\w*\\b'\n","    custom = r'(\\x89|û|ó)'\n","    emoji = re.compile(r\"[\\U0001F600-\\U0001F64F\"  # Emoticons\n","                   \"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n","                   \"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n","                   \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n","                   \"\\U00002702-\\U000027B0\"  # Dingbats\n","                   \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n","                   \"]+\", \n","                  flags=re.UNICODE)\n","    \n","    extensions = ['com', 'fr', 'org', 'net', 'edu', 'gov', 'uk', 'de', 'jp', 'au', 'ca', 'us', 'info', 'biz', 'co']\n","    web_site = r'\\b\\S+\\.(?:' + '|'.join(extensions) + r')\\b'\n","    \n","    patterns = [url,\n","                num_in_words,\n","               html,\n","               emoji,\n","               web_site,\n","               rf'[{re.escape(special_char)}]']\n","    \n","    text = del_patterns(patterns, text)\n","    \n","    for word in correction_dict:\n","        text = text.replace(word, correction_dict[word])\n","        \n","    return lemmatize(''.join(text))\n","\n","def get_duplicate_text(data : pd.DataFrame, name_col_text: str):\n","    \"\"\"\n","        get the text dulpicated\n","        \n","        Args:\n","            data (pd.DataFrame): \n","                data we want to extract unique duplicated text\n","                \n","            name_col_text (str):\n","                the name of the column in the dataframe that contains the text\n","                \n","        Returns:\n","            the list of text \n","    \"\"\"\n","    list_text = []\n","    for value, txt in zip(data[name_col_text].value_counts().tolist(), data[name_col_text].value_counts().index):\n","        if value>1:\n","            list_text.append(txt)\n","            \n","    return list_text\n","\n","def get_index_different_target(train, texts):\n","    \"\"\"\n","        Among the texts duplicated we look after\n","        those which do not have the same target\n","\n","        Args:\n","            train (pd.DataFrame): \n","                the dataframe we want to extract the index\n","                \n","            texts (list):\n","                list of duplicated text\n","                \n","        Returns:\n","            the list of index\n","    \"\"\"\n","    list_index = {}\n","    for i, text in enumerate(texts):\n","        list_index[\"text{}\".format(i)] = {\"index\" : [],\n","                                          \"target\" : []}\n","        \n","        for ind, data_txt in zip(train.text.index, train.text.tolist()):\n","                if data_txt in text:\n","                    list_index[\"text{}\".format(i)][\"index\"].append(ind)\n","                    list_index[\"text{}\".format(i)][\"target\"].append(train.target.loc[ind])\n","                    \n","    new_list_index = {}\n","    for j in list_index.keys():\n","        if len(Counter(list_index[j][\"target\"]))>1:\n","            new_list_index[j] = list_index[j]\n","            # print(list_index[j][\"index\"], train.loc[list_index[j][\"index\"][0]].text, end= '\\n')\n","    return new_list_index\n","\n","def hard_coded_targetting(train):\n","    \"\"\"\n","        hard coded function to associate correct target \n","        for duplicates text with differents target\n","\n","    Args:\n","        train (pd.DataFrame): \n","            the train dataframe\n","            \n","    Returns:\n","        the train with duplicates text and correct targets\n","    \"\"\"\n","    \n","    dict_target ={\n","                3240: 0, 3243: 0, 3248: 0, 3251: 0, 3261: 0, 3266: 0,\n","                4284: 0, 4286: 0, 4292: 0, 4304: 0, 4309: 0, 4318: 0,\n","                6091: 0, 6094: 0, 6103: 0, 6123: 0,\n","                610: 0, 624: 0, 630: 0, 634: 0,\n","                2830: 0, 2831: 0, 2832: 0, 2833: 0,\n","                3985: 0, 4013: 0, 4019: 0,\n","                4285: 0, 4305: 0, 4313: 0,\n","                4290: 0, 4299: 0, 4312: 0,\n","                4597: 1, 4605: 1, 4618: 1, 4623: 1, 4631: 1,\n","                4221: 0, 4239: 0, 4244: 0,\n","                4232: 0, 4235: 0,\n","                4379: 1, 4381: 1,\n","                5620: 1, 5641: 1,\n","                1197: 0, 1331: 0,\n","                6614: 0, 6616: 0,\n","                1221: 0, 1349: 0,\n","                1214: 0, 1365: 0,\n","                4306: 0, 4320: 0,\n","                4285: 0, 4294: 0, 4305: 0, 4308: 0, 4313: 0\n","                }\n","\n","    \n","    for ind in dict_target.keys():\n","        train.loc[ind, \"target\"] = dict_target[ind]\n","        \n","    return train\n","\n","def get_unk_words(data, glove_dict, label_text_column):\n","    \"\"\"\n","        Get unk word taht are not in glove after data cleaning\n","\n","        Args:\n","            data (list): \n","                list of texts\n","                \n","            glove_dict (dict): \n","                glove word dict\n","                \n","        Returns:\n","            the list of unk word\n","    \"\"\"\n","    \n","    list_unk = []\n","    tot_words = 0\n","    \n","    for text in data:\n","        # make sure there is a text\n","        if isinstance(text, str):\n","            for word in text.split():\n","                tot_words += 1\n","                if word not in glove_dict:\n","                    list_unk.append(word)\n","                    \n","    print(f\"there a total of {tot_words} for {len(list(Counter(list_unk).keys()))} unk words that are not in glove\")\n","    \n","    return list_unk\n"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Ce texte contient des mots comme , ûó, simpletext, et d'autres mots normaux.\n"]}],"source":["def remove_words_with_digits(text):\n","    pattern = r'\\b\\w*\\d+\\w*\\b'\n","    cleaned_text = re.sub(pattern, '', text)\n","    #cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n","    return cleaned_text\n","\n","text = \"Ce texte contient des mots comme e10march, \\x89ûó, simpletext, et d'autres mots normaux.\"\n","cleaned_text = remove_words_with_digits(text)\n","print(cleaned_text)"]},{"cell_type":"markdown","metadata":{},"source":["# Glove embedding"]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:11:39.287955Z","iopub.status.busy":"2024-06-30T19:11:39.287527Z","iopub.status.idle":"2024-06-30T19:11:50.047521Z","shell.execute_reply":"2024-06-30T19:11:50.046017Z","shell.execute_reply.started":"2024-06-30T19:11:39.287909Z"},"trusted":true},"outputs":[],"source":["embed_dict = {}\n","word_dict = {}\n","with open(\"/Users/avicenne/Documents/python/NLP/Glove/glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        word_dict[word] = word\n","        vector = np.asarray(values[1:], \"float32\")\n","        embed_dict[word] = vector"]},{"cell_type":"markdown","metadata":{},"source":["# Some corrected words related to disasters"]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[],"source":["hard_coded_dict = {\n","    \"goooooooaaaaaal\": \"goal\",\n","    \"looooool\": \"lol\",\n","    \"newsnigeria\": \"news nigeria\",\n","    \"southridgelife\": \"south ridgde life\",\n","    \"nowplaying\": \"now playing\",\n","    \"û÷hijacker\": \"hijacker\",\n","    \"phdsquares\": \"phd squares\",\n","    \"personalinjury\": \"personal injury\",\n","    \"caraccidentlawyer\": \"car accident lawyer\",\n","    \"accidentwho\": \"accident who\",\n","    \"truckcrash\": \"truck crash\",\n","    \"crashgt\": \"crash\",\n","    \"damagenhs\": \"damages\",\n","    \"damagewpd\": \"damaged\",\n","    \"nearfatal\":  \"near fatal\",\n","    \"southaccident\": \"south accident\",\n","    \"measuresarrestpastornganga\": \"measures arrest pastor and gang\",\n","    \"aftershockdelo\": \"after shock\",\n","    \"lifehacks\": \"lifehacker\",\n","    \"onfireanders\": \"on fire anders\",\n","    \"aftershockorg\": \"after shock\",\n","    \"butthe\": \"battle\",\n","    \"breakfastone\": \"break fast one\",\n","    \"uptotheminute\": \"up to the minute\",\n","    \"warmbodies\": \"warm bodies\",\n","    \"aggressif\": \"aggressive\",\n","    \"allah\": \"god\"\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Duplicates text with differents target"]},{"cell_type":"code","execution_count":215,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["number of text duplicates with different targets : 19\n","number of text duplicates with different targets : 0\n"]}],"source":["train = pd.read_csv(\"/Users/avicenne/Documents/python/Project-github/Kaggle/nlp-getting-started/train.csv\")\n","test = pd.read_csv(\"/Users/avicenne/Documents/python/Project-github/Kaggle/nlp-getting-started/test.csv\")\n","\n","list_text_before = get_duplicate_text(train, 'text')\n","print(\"number of text duplicates with different targets :\",len(get_index_different_target(train, list_text_before)))\n","\n","train = hard_coded_targetting(train)\n","list_text_after = get_duplicate_text(train, 'text')\n","print(\"number of text duplicates with different targets :\",len(get_index_different_target(train, list_text_after)))\n","\n","X_test = test[\"text\"].apply(lambda x: clean_text(x, hard_coded_dict)).tolist()\n","id_test = test[\"id\"]\n","\n","X_train = train[\"text\"].apply(lambda x: clean_text(x, hard_coded_dict)).tolist()\n","Y_train = train[\"target\"].tolist()"]},{"cell_type":"markdown","metadata":{},"source":["# Display word that are not in glove"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[],"source":["for i in word_dict:\n","    if \"lmao\" in i:\n","        print(i)"]},{"cell_type":"code","execution_count":219,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["there a total of 103771 for 4756 unk words that are not in glove\n"]}],"source":["unk_words = list(Counter(get_unk_words(X_train, word_dict, \"text\")).keys())"]},{"cell_type":"code","execution_count":202,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4781/4781 [05:31<00:00, 14.41it/s]\n"]}],"source":["from tqdm import tqdm\n","\n","from autocorrect import Speller\n","spell = Speller(lang=\"en\")\n","\n","dict_correction= {}\n","\n","for word in tqdm(unk_words):\n","    if word not in dict_correction:\n","        dict_correction[word] = [spell(word)]\n","    else:\n","        dict_correction[word].append(spell(word))"]},{"cell_type":"code","execution_count":204,"metadata":{},"outputs":[],"source":["\"\"\" save the dict to take a look and build the hard coded correction dict\n","\n","import json\n","file_path = 'dict_correction.json'\n","\n","# Sauvegarder le dictionnaire dans un fichier JSON\n","with open(file_path, 'w') as file:\n","    json.dump(dict_correction, file, indent=4)\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# TFIDF"]},{"cell_type":"markdown","metadata":{},"source":["TF-IDF (Term Frequency-Inverse Document Frequency): this is a widely-used method that calculates a score for each term by combining a value that depends on its frequency of appearance in a text (term frequency) and a second value that depends on its appearance in all texts. The advantages are that the method is simple to implement and therefore effective, filters out common terms by lowering their scores, and highlights important rare terms. However, sensitivity to rare terms can exaggerate the score, semantic relations are not taken into account and the method loses effectiveness on long texts."]},{"cell_type":"code","execution_count":220,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:11:28.016821Z","iopub.status.busy":"2024-06-30T19:11:28.016420Z","iopub.status.idle":"2024-06-30T19:11:28.248029Z","shell.execute_reply":"2024-06-30T19:11:28.246734Z","shell.execute_reply.started":"2024-06-30T19:11:28.016792Z"},"trusted":true},"outputs":[],"source":["tfidf_vectorizer = TfidfVectorizer() \n","\n","tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n","tfidf_test_vectors = tfidf_vectorizer.transform(X_test)"]},{"cell_type":"code","execution_count":221,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:11:38.120584Z","iopub.status.busy":"2024-06-30T19:11:38.120100Z","iopub.status.idle":"2024-06-30T19:11:38.132884Z","shell.execute_reply":"2024-06-30T19:11:38.131728Z","shell.execute_reply.started":"2024-06-30T19:11:38.120547Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(5329, 15109)\n"]}],"source":["x_train_tfidf, x_val_tfidf, y_train, y_val = train_test_split(tfidf_train_vectors,\n","                                                  Y_train,\n","                                                  test_size = 0.3, random_state = 42)\n","print(x_train_tfidf.shape)"]},{"cell_type":"markdown","metadata":{},"source":["shape data : texts = [train, val, test]\n","             labels = [train, val, None]"]},{"cell_type":"code","execution_count":222,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:11:50.050192Z","iopub.status.busy":"2024-06-30T19:11:50.049706Z","iopub.status.idle":"2024-06-30T19:11:50.060543Z","shell.execute_reply":"2024-06-30T19:11:50.059438Z","shell.execute_reply.started":"2024-06-30T19:11:50.050149Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5329\n"]}],"source":["x_train, x_val, y_train, y_val = train_test_split(X_train,\n","                                                  Y_train,\n","                                                  test_size = 0.3, random_state = 42)\n","print(len(x_train))"]},{"cell_type":"code","execution_count":223,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:11:50.062215Z","iopub.status.busy":"2024-06-30T19:11:50.061842Z","iopub.status.idle":"2024-06-30T19:11:50.079844Z","shell.execute_reply":"2024-06-30T19:11:50.078502Z","shell.execute_reply.started":"2024-06-30T19:11:50.062186Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5329\n","2284\n","3263\n"]}],"source":["texts = [i for i in x_train]\n","texts += [i for i in x_val]\n","texts += [i for i in X_test]\n","\n","print(len(x_train))\n","print(len(x_val))\n","print(len(X_test))"]},{"cell_type":"markdown","metadata":{},"source":["# create vocabulary"]},{"cell_type":"markdown","metadata":{},"source":["# Keras"]},{"cell_type":"code","execution_count":224,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:11:50.082967Z","iopub.status.busy":"2024-06-30T19:11:50.082576Z","iopub.status.idle":"2024-06-30T19:12:04.211915Z","shell.execute_reply":"2024-06-30T19:12:04.210534Z","shell.execute_reply.started":"2024-06-30T19:11:50.082915Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 19312 unique tokens.\n","Shape of data tensor: (10876, 250)\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","maxlen = 250\n","max_words = 100000\n","embedding_size=150\n","lr = 1e-3\n","lr_d = 0\n","\n","# create voc\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","data = pad_sequences(sequences, maxlen=maxlen)\n","#labels = np.asarray(labels)\n","print('Shape of data tensor:', data.shape)\n","\n","# split data\n","x_train_keras = data[0:5329]\n","x_val_keras = data[5329 : 5329+2284]\n","x_test_keras = data[5329+2284 : 5329+2284+3263]"]},{"cell_type":"code","execution_count":225,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:12:04.214233Z","iopub.status.busy":"2024-06-30T19:12:04.213320Z","iopub.status.idle":"2024-06-30T19:12:04.220254Z","shell.execute_reply":"2024-06-30T19:12:04.219076Z","shell.execute_reply.started":"2024-06-30T19:12:04.214197Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of train tensor: (5329, 250)\n","Shape of validate tensor: (2284, 250)\n","Shape of test tensor: (3263, 250)\n"]}],"source":["print('Shape of train tensor:', x_train_keras.shape)\n","print('Shape of validate tensor:', x_val_keras.shape)\n","print('Shape of test tensor:', x_test_keras.shape)"]},{"cell_type":"markdown","metadata":{},"source":["build embedding matrix"]},{"cell_type":"code","execution_count":226,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:12:04.222688Z","iopub.status.busy":"2024-06-30T19:12:04.222243Z","iopub.status.idle":"2024-06-30T19:12:04.392079Z","shell.execute_reply":"2024-06-30T19:12:04.390768Z","shell.execute_reply.started":"2024-06-30T19:12:04.222644Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(400000, 100)\n"]}],"source":["def get_embedding_matrix(word_index, embed_dict):\n","\n","    embedding_dim = 100 #length of vect word\n","    max_words = 400000  #number of embedded word we take from glove (out of 400 000 words vect)\n","    embedding_matrix = np.zeros((max_words, embedding_dim)) # matrix vect words\n","    for word, ind in word_index.items():\n","        if ind < max_words:\n","            embedding_vector = embed_dict.get(word)\n","            if embedding_vector is not None:\n","                embedding_matrix[ind] = embedding_vector\n","    print(embedding_matrix.shape)\n","    return embedding_matrix\n","    \n","embedding_matrix_keras = get_embedding_matrix(word_index, embed_dict)"]},{"cell_type":"markdown","metadata":{},"source":["# Bert"]},{"cell_type":"code","execution_count":227,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:12:04.393771Z","iopub.status.busy":"2024-06-30T19:12:04.393416Z","iopub.status.idle":"2024-06-30T19:12:16.740278Z","shell.execute_reply":"2024-06-30T19:12:16.739035Z","shell.execute_reply.started":"2024-06-30T19:12:04.393739Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 30522 unique tokens.\n","Shape of data tensor: (10876, 250)\n"]}],"source":["import tensorflow as tf\n","from transformers import BertTokenizer\n","\n","# Initialisation du tokenizer BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenizer les textes avec padding et truncation\n","def tokenize_(texts, tokenizer, maxlen):\n","    encoded_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=250, return_tensors='tf')\n","    return encoded_inputs['input_ids']\n","\n","data = tokenize_(texts, tokenizer, maxlen)\n","\n","# Create dict word_unique_vocab -> index\n","bert_word_index = {word: idx for idx, word in enumerate(tokenizer.get_vocab().keys())}\n","\n","print(f'Found {len(bert_word_index)} unique tokens.')\n","print('Shape of data tensor:', data.shape)\n","\n","# split data\n","x_train_bert = data[0:5329]\n","x_val_bert = data[5329 : 5329+2284]\n","x_test_bert = data[5329+2284 : 5329+2284+3263]"]},{"cell_type":"code","execution_count":228,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:12:16.742305Z","iopub.status.busy":"2024-06-30T19:12:16.741630Z","iopub.status.idle":"2024-06-30T19:12:16.748621Z","shell.execute_reply":"2024-06-30T19:12:16.747373Z","shell.execute_reply.started":"2024-06-30T19:12:16.742272Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of train tensor: (5329, 250)\n","Shape of validate tensor: (2284, 250)\n","Shape of test tensor: (3263, 250)\n"]}],"source":["print('Shape of train tensor:', x_train_bert.shape)\n","print('Shape of validate tensor:', x_val_bert.shape)\n","print('Shape of test tensor:', x_test_bert.shape)"]},{"cell_type":"code","execution_count":229,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:12:16.751097Z","iopub.status.busy":"2024-06-30T19:12:16.750593Z","iopub.status.idle":"2024-06-30T19:12:16.817447Z","shell.execute_reply":"2024-06-30T19:12:16.816304Z","shell.execute_reply.started":"2024-06-30T19:12:16.751056Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(400000, 100)\n"]}],"source":["embedding_matrix_bert = get_embedding_matrix(bert_word_index, embed_dict)"]},{"cell_type":"markdown","metadata":{},"source":["# CNN model"]},{"cell_type":"code","execution_count":230,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:12:16.820469Z","iopub.status.busy":"2024-06-30T19:12:16.820117Z","iopub.status.idle":"2024-06-30T19:12:16.993914Z","shell.execute_reply":"2024-06-30T19:12:16.992828Z","shell.execute_reply.started":"2024-06-30T19:12:16.820440Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/avicenne/anaconda3/envs/ultra/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_2\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D\n","from keras.optimizers import Adam\n","\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=400000, output_dim=100, input_length=250))\n","model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(10, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()\n","model.build(input_shape=(None, 250))"]},{"cell_type":"code","execution_count":231,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:12:16.995610Z","iopub.status.busy":"2024-06-30T19:12:16.995206Z","iopub.status.idle":"2024-06-30T19:12:17.171371Z","shell.execute_reply":"2024-06-30T19:12:17.170141Z","shell.execute_reply.started":"2024-06-30T19:12:16.995581Z"},"trusted":true},"outputs":[],"source":["# Définir les poids de la couche d'Embedding\n","model.layers[0].set_weights([np.array(embedding_matrix_keras)])\n","model.layers[0].trainable = False\n","\n","#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n","model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":232,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:12:36.110137Z","iopub.status.busy":"2024-06-30T19:12:36.109711Z","iopub.status.idle":"2024-06-30T19:13:59.318558Z","shell.execute_reply":"2024-06-30T19:13:59.317258Z","shell.execute_reply.started":"2024-06-30T19:12:36.110103Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.5723 - loss: 0.6792 - val_accuracy: 0.7531 - val_loss: 0.6068\n","Epoch 2/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.7449 - loss: 0.5788 - val_accuracy: 0.7579 - val_loss: 0.5287\n","Epoch 3/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.7683 - loss: 0.5068 - val_accuracy: 0.7890 - val_loss: 0.4723\n","Epoch 4/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.7864 - loss: 0.4646 - val_accuracy: 0.7968 - val_loss: 0.4571\n","Epoch 5/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.8036 - loss: 0.4370 - val_accuracy: 0.8047 - val_loss: 0.4493\n","Epoch 6/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 0.8206 - loss: 0.4075 - val_accuracy: 0.7973 - val_loss: 0.4565\n","Epoch 7/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.8157 - loss: 0.4101 - val_accuracy: 0.8025 - val_loss: 0.4489\n","Epoch 8/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.8359 - loss: 0.3791 - val_accuracy: 0.8082 - val_loss: 0.4474\n","Epoch 9/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.8490 - loss: 0.3589 - val_accuracy: 0.7938 - val_loss: 0.4575\n","Epoch 10/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.8537 - loss: 0.3413 - val_accuracy: 0.8095 - val_loss: 0.4484\n","Epoch 11/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.8621 - loss: 0.3184 - val_accuracy: 0.7920 - val_loss: 0.4675\n","Epoch 12/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.8729 - loss: 0.3105 - val_accuracy: 0.8021 - val_loss: 0.4729\n","Epoch 13/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.8754 - loss: 0.2999 - val_accuracy: 0.8012 - val_loss: 0.4665\n","Epoch 14/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.9018 - loss: 0.2651 - val_accuracy: 0.8060 - val_loss: 0.4657\n","Epoch 15/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.9099 - loss: 0.2447 - val_accuracy: 0.7964 - val_loss: 0.4759\n","Epoch 16/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.9214 - loss: 0.2293 - val_accuracy: 0.7968 - val_loss: 0.4831\n","Epoch 17/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.9313 - loss: 0.2156 - val_accuracy: 0.7933 - val_loss: 0.4933\n","Epoch 18/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.9360 - loss: 0.2010 - val_accuracy: 0.7955 - val_loss: 0.5020\n","Epoch 19/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.9434 - loss: 0.1858 - val_accuracy: 0.7947 - val_loss: 0.5102\n","Epoch 20/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 143ms/step - accuracy: 0.9501 - loss: 0.1726 - val_accuracy: 0.7933 - val_loss: 0.5203\n"]}],"source":["history = model.fit(np.array(x_train_keras), np.array(y_train),\n","                    epochs=20,\n","                    batch_size=512,\n","                    validation_data=(np.array(x_val_keras), np.array(y_val)))"]},{"cell_type":"code","execution_count":240,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.5239 - loss: 1.1202 - val_accuracy: 0.5797 - val_loss: 0.7679\n","Epoch 2/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.5726 - loss: 0.7056 - val_accuracy: 0.5845 - val_loss: 0.6714\n","Epoch 3/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.6234 - loss: 0.6512 - val_accuracy: 0.6077 - val_loss: 0.6602\n","Epoch 4/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.6690 - loss: 0.6315 - val_accuracy: 0.6243 - val_loss: 0.6535\n","Epoch 5/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 0.6852 - loss: 0.6139 - val_accuracy: 0.6397 - val_loss: 0.6424\n","Epoch 6/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.7103 - loss: 0.5854 - val_accuracy: 0.6484 - val_loss: 0.6331\n","Epoch 7/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - accuracy: 0.7288 - loss: 0.5620 - val_accuracy: 0.6489 - val_loss: 0.6252\n","Epoch 8/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.7530 - loss: 0.5369 - val_accuracy: 0.6686 - val_loss: 0.6170\n","Epoch 9/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.7749 - loss: 0.5086 - val_accuracy: 0.6712 - val_loss: 0.6113\n","Epoch 10/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.7990 - loss: 0.4736 - val_accuracy: 0.6729 - val_loss: 0.6097\n","Epoch 11/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.8178 - loss: 0.4489 - val_accuracy: 0.6804 - val_loss: 0.6078\n","Epoch 12/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - accuracy: 0.8326 - loss: 0.4249 - val_accuracy: 0.6821 - val_loss: 0.6096\n","Epoch 13/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.8497 - loss: 0.3973 - val_accuracy: 0.6821 - val_loss: 0.6103\n","Epoch 14/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - accuracy: 0.8608 - loss: 0.3700 - val_accuracy: 0.6813 - val_loss: 0.6142\n","Epoch 15/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.8715 - loss: 0.3490 - val_accuracy: 0.6799 - val_loss: 0.6190\n","Epoch 16/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.9073 - loss: 0.3094 - val_accuracy: 0.6870 - val_loss: 0.6251\n","Epoch 17/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9090 - loss: 0.2953 - val_accuracy: 0.6865 - val_loss: 0.6304\n","Epoch 18/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.9254 - loss: 0.2677 - val_accuracy: 0.6817 - val_loss: 0.6373\n","Epoch 19/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9343 - loss: 0.2469 - val_accuracy: 0.6778 - val_loss: 0.6487\n","Epoch 20/20\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9477 - loss: 0.2232 - val_accuracy: 0.6935 - val_loss: 0.6632\n"]}],"source":["history = model.fit(np.array(x_train_bert), np.array(y_train),\n","                    epochs=20,\n","                    batch_size=512,\n","                    validation_data=(np.array(x_val_bert), np.array(y_val)))"]},{"cell_type":"markdown","metadata":{},"source":["# Classification"]},{"cell_type":"code","execution_count":233,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:14:05.820568Z","iopub.status.busy":"2024-06-30T19:14:05.820166Z","iopub.status.idle":"2024-06-30T19:14:08.862133Z","shell.execute_reply":"2024-06-30T19:14:08.861001Z","shell.execute_reply.started":"2024-06-30T19:14:05.820535Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy :  0.7788966725043783\n"]}],"source":["from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","XGB = XGBClassifier()\n","    \n","\n","XGB.fit(x_train_tfidf, y_train)\n","\n","predictions_xgb = XGB.predict(x_val_tfidf)\n","\n","print(\"Accuracy : \",  accuracy_score(y_val, predictions_xgb))"]},{"cell_type":"markdown","metadata":{},"source":["# Bagging"]},{"cell_type":"code","execution_count":234,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:14:08.864266Z","iopub.status.busy":"2024-06-30T19:14:08.863910Z","iopub.status.idle":"2024-06-30T19:14:08.871518Z","shell.execute_reply":"2024-06-30T19:14:08.870318Z","shell.execute_reply.started":"2024-06-30T19:14:08.864239Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import BaggingClassifier\n","\n","def bag_model(model, x_train, y_train, x_val, y_val, cross_val):\n","    bag = BaggingClassifier(estimator=model,\n","                            n_estimators=10, \n","                            random_state=0).fit(x_train, y_train)\n","\n","    predictions_bag = bag.predict(x_val)\n","\n","    print(\"accuracy bag: \",  accuracy_score(y_val, predictions_bag))\n","\n","    if cross_val:\n","        # Utiliser la validation croisée pour évaluer la performance du modèle\n","        scores = cross_val_score(bag, x_train, y_train, cv=5, scoring='accuracy')\n","        print(\"Cross-validation accuracy scores: \", scores)\n","        print(\"Mean cross-validation accuracy: \", scores.mean())\n","    return bag"]},{"cell_type":"code","execution_count":236,"metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:09:39.616875Z","iopub.status.idle":"2024-06-30T19:09:39.617307Z","shell.execute_reply":"2024-06-30T19:09:39.617133Z","shell.execute_reply.started":"2024-06-30T19:09:39.617116Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy bag:  0.7880910683012259\n"]}],"source":["bag_xgb = bag_model(XGBClassifier(random_state=42), x_train_tfidf, y_train, x_val_tfidf, y_val, False)"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:09:39.618475Z","iopub.status.idle":"2024-06-30T19:09:39.618863Z","shell.execute_reply":"2024-06-30T19:09:39.618690Z","shell.execute_reply.started":"2024-06-30T19:09:39.618674Z"},"trusted":true},"outputs":[],"source":["#bag_cat = bag_model(CatBoostClassifier(random_state=42, verbose=0), x_train, y_train, x_val, y_val, False)"]},{"cell_type":"markdown","metadata":{},"source":["# Voting"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:09:40.926572Z","iopub.status.busy":"2024-06-30T19:09:40.926180Z","iopub.status.idle":"2024-06-30T19:09:40.966965Z","shell.execute_reply":"2024-06-30T19:09:40.965355Z","shell.execute_reply.started":"2024-06-30T19:09:40.926535Z"},"trusted":true},"outputs":[],"source":["xgb = XGBClassifier(random_state=42)\n","cat = CatBoostClassifier(random_state=42, verbose=0)\n","lgb = LGBMClassifier(random_state = 42, verbose=-1)\n","\n","def voting_models(models, x_train, y_train, x_val, y_val, cross_val):\n","    \n","    voting = VotingClassifier(estimators=models,\n","                                    voting='soft')\n","\n","    voting.fit(x_train, y_train)\n","\n","    predictions_boost = voting.predict(x_val)\n","\n","    print(\"accuracy boost: \",  accuracy_score(y_val, predictions_boost))\n","\n","    if cross_val:\n","        # Utiliser la validation croisée pour évaluer la performance du modèle\n","        scores = cross_val_score(voting, x_train, y_train, cv=5, scoring='accuracy')\n","        print(\"Cross-validation accuracy scores: \", scores)\n","        print(\"Mean cross-validation accuracy: \", scores.mean())\n","    \n","    return voting\n","\n","models = [('xgb', xgb), ('cat', cat), ('lgb', lgb)]\n","\n","# vot = voting_models(models, x_train, y_train, x_val, y_val, False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":241,"metadata":{"execution":{"iopub.execute_input":"2024-06-30T19:42:54.443473Z","iopub.status.busy":"2024-06-30T19:42:54.443057Z","iopub.status.idle":"2024-06-30T19:42:55.253063Z","shell.execute_reply":"2024-06-30T19:42:55.251968Z","shell.execute_reply.started":"2024-06-30T19:42:54.443443Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]}],"source":["def submission(model, test, Id, cnn):\n","    pred = model.predict(test)\n","    \n","    if cnn:\n","        pred_c = []\n","        for i in pred:\n","            if i>0.6:\n","                pred_c.append(1)\n","            else:\n","                pred_c.append(0)\n","        pred = pred_c\n","\n","    submission = pd.DataFrame({'id': Id,\n","                               'target': pred})\n","\n","    # Save submission to a CSV file\n","    submission.to_csv('submission.csv', index=False)\n","    \n","submission(model, np.array(x_test_keras), id_test, True)"]},{"cell_type":"code","execution_count":238,"metadata":{},"outputs":[],"source":["submission(bag_xgb, tfidf_test_vectors, id_test, False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":869809,"sourceId":17777,"sourceType":"competition"},{"datasetId":5260894,"sourceId":8757105,"sourceType":"datasetVersion"},{"sourceId":185154041,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
