{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":8757105,"sourceType":"datasetVersion","datasetId":5260894},{"sourceId":185154041,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport utilities\n\n# text processing\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('wordnet', '/nltk_data')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer \n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# models\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import VotingClassifier\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-28T16:41:32.195774Z","iopub.execute_input":"2024-06-28T16:41:32.196216Z","iopub.status.idle":"2024-06-28T16:41:33.689848Z","shell.execute_reply.started":"2024-06-28T16:41:32.196180Z","shell.execute_reply":"2024-06-28T16:41:33.688494Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# We import glove file in order to explore new method based on its word vect representation","metadata":{}},{"cell_type":"code","source":"\"\"\"embed_dict = {}\nword_dict = {}\nwith open(\"/kaggle/input/glove-100d/glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        word_dict[word] = word\n        vector = np.asarray(values[1:], \"float32\")\n        embed_dict[word] = vector\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:43.242956Z","iopub.execute_input":"2024-06-26T10:41:43.243667Z","iopub.status.idle":"2024-06-26T10:41:43.251751Z","shell.execute_reply.started":"2024-06-26T10:41:43.243624Z","shell.execute_reply":"2024-06-26T10:41:43.250689Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'embed_dict = {}\\nword_dict = {}\\nwith open(\"/kaggle/input/glove-100d/glove.6B.100d.txt\", \\'r\\', encoding=\"utf-8\") as f:\\n    for line in f:\\n        values = line.split()\\n        word = values[0]\\n        word_dict[word] = word\\n        vector = np.asarray(values[1:], \"float32\")\\n        embed_dict[word] = vector'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Function to clean the text","metadata":{}},{"cell_type":"code","source":"def remove_stop_word(text):\n    \"\"\"\n        remove stop word in text, \n        such as the, as, is...\n        \n        Args : \n            text (str) :\n                the text we want to remove stopword\n                \n        returns :\n            text without stop word\n    \"\"\"\n    \n    STOPWORDS = set(stopwords.words('english'))\n\n    return \" \".join([word for word in text.split() if word not in STOPWORDS])\n    \n\ndef del_pattern(pattern, text):\n    \"\"\"\n        Del specific pattern in text\n        \n        Args :\n            pattern (str) : \n                the pattern we want to remove\n            \n            text (str) :\n                the text we want to clean\n        \n        Returns :\n            text without pattern\n    \"\"\"\n    \n    return re.sub(pattern, '', text)\n\ndef lemmatize(text):\n    \"\"\"\n        Lemmatize text : \n        bats -> bat\n        cats -> cat\n        \n        Args :\n            text (str) :\n                the text we want to lemmatize\n                \n        returns :\n            the text lemmatize\n    \"\"\"\n    \n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(w) for w in text.split()])\n\n\ndef del_patterns(patterns, text):\n    \"\"\"\n        Del specific patterns in text\n        \n        Args :\n            pattern (list) : \n                the patterns we want to remove\n            \n            text (str) :\n                the text we want to clean\n        \n        Returns :\n            text without patterns\n    \"\"\"\n    \n    for pattern in patterns:\n        text = del_pattern(pattern, text)\n    \n    return text\n\ndef clean_text(text):\n    \"\"\" \n        The goal og this function is to clean a text by removing stopwords, special caracters...\n\n        Args:\n            text (str): \n                text we want to clean\n\n        Returns:\n            the text cleaned\n    \"\"\"\n    \n    text = str(text).lower()\n    \n    special_char = r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n    html = re.compile(r\"<.*?>\")    \n    num_in_words = r'\\d+'\n    emoji = re.compile(r\"[\\U0001F600-\\U0001F64F\"  # Emoticons\n                   \"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n                   \"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n                   \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n                   \"\\U00002702-\\U000027B0\"  # Dingbats\n                   \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n                   \"]+\", \n                  flags=re.UNICODE)\n    \n    patterns = [rf'[{re.escape(special_char)}]', \n               url,\n               html,\n               emoji,\n               num_in_words]\n    \n    text = del_patterns(patterns, text)\n    \n    return lemmatize(text)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T16:41:41.078415Z","iopub.execute_input":"2024-06-28T16:41:41.078984Z","iopub.status.idle":"2024-06-28T16:41:41.094929Z","shell.execute_reply.started":"2024-06-28T16:41:41.078941Z","shell.execute_reply":"2024-06-28T16:41:41.093933Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T16:41:42.648412Z","iopub.execute_input":"2024-06-28T16:41:42.648808Z","iopub.status.idle":"2024-06-28T16:41:42.710035Z","shell.execute_reply.started":"2024-06-28T16:41:42.648779Z","shell.execute_reply":"2024-06-28T16:41:42.708954Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_test = test[\"text\"].apply(clean_text).tolist()\nid_test = test[\"id\"]\n\nX_train = train[\"text\"].apply(clean_text).tolist()\nY_train = train[\"target\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T16:41:44.034479Z","iopub.execute_input":"2024-06-28T16:41:44.035469Z","iopub.status.idle":"2024-06-28T16:41:48.127709Z","shell.execute_reply.started":"2024-06-28T16:41:44.035430Z","shell.execute_reply":"2024-06-28T16:41:48.126502Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# TFIDF","metadata":{}},{"cell_type":"markdown","source":"TF-IDF (Term Frequency-Inverse Document Frequency): this is a widely-used method that calculates a score for each term by combining a value that depends on its frequency of appearance in a text (term frequency) and a second value that depends on its appearance in all texts. The advantages are that the method is simple to implement and therefore effective, filters out common terms by lowering their scores, and highlights important rare terms. However, sensitivity to rare terms can exaggerate the score, semantic relations are not taken into account and the method loses effectiveness on long texts.","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer() \n\ntfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test_vectors = tfidf_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T16:42:17.401565Z","iopub.execute_input":"2024-06-28T16:42:17.402460Z","iopub.status.idle":"2024-06-28T16:42:17.720146Z","shell.execute_reply.started":"2024-06-28T16:42:17.402422Z","shell.execute_reply":"2024-06-28T16:42:17.719157Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(tfidf_train_vectors,\n                                                  Y_train,\n                                                  test_size = 0.3, random_state = 42)\nprint(x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T16:42:30.778516Z","iopub.execute_input":"2024-06-28T16:42:30.778905Z","iopub.status.idle":"2024-06-28T16:42:30.791943Z","shell.execute_reply.started":"2024-06-28T16:42:30.778877Z","shell.execute_reply":"2024-06-28T16:42:30.790724Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"(5329, 20241)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nXGB = XGBClassifier()\n    \n\nXGB.fit(x_train, y_train)\n\npredictions_xgb = XGB.predict(x_val)\n\nprint(\"Accuracy : \",  accuracy_score(y_val, predictions_xgb))","metadata":{"execution":{"iopub.status.busy":"2024-06-28T16:42:35.033262Z","iopub.execute_input":"2024-06-28T16:42:35.034392Z","iopub.status.idle":"2024-06-28T16:42:42.784002Z","shell.execute_reply.started":"2024-06-28T16:42:35.034336Z","shell.execute_reply":"2024-06-28T16:42:42.783031Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Accuracy :  0.7692644483362522\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Bagging","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\ndef bag_model(model, x_train, y_train, x_val, y_val):\n    bag = BaggingClassifier(estimator=model,\n                            n_estimators=10, \n                            random_state=0).fit(x_train, y_train)\n\n    predictions_bag = bag.predict(x_val)\n\n    print(\"accuracy bag: \",  accuracy_score(y_val, predictions_bag))\n\n    # Utiliser la validation croisée pour évaluer la performance du modèle\n    scores = cross_val_score(bag, x_train, y_train, cv=5, scoring='accuracy')\n    print(\"Cross-validation accuracy scores: \", scores)\n    print(\"Mean cross-validation accuracy: \", scores.mean())\n    return bag","metadata":{"execution":{"iopub.status.busy":"2024-06-28T16:42:50.337606Z","iopub.execute_input":"2024-06-28T16:42:50.338021Z","iopub.status.idle":"2024-06-28T16:42:50.346252Z","shell.execute_reply.started":"2024-06-28T16:42:50.337989Z","shell.execute_reply":"2024-06-28T16:42:50.344907Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"bag_xgb = bag_model(XGBClassifier(random_state=42), x_train, y_train, x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:41:47.667839Z","iopub.execute_input":"2024-06-26T10:41:47.668617Z","iopub.status.idle":"2024-06-26T10:43:24.768779Z","shell.execute_reply.started":"2024-06-26T10:41:47.668584Z","shell.execute_reply":"2024-06-26T10:43:24.767969Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"accuracy bag:  0.7670753064798599\nCross-validation accuracy scores:  [0.76641651 0.7804878  0.76172608 0.75984991 0.76901408]\nMean cross-validation accuracy:  0.7674988769389318\n","output_type":"stream"}]},{"cell_type":"code","source":"bag_cat = bag_model(CatBoostClassifier(random_state=42, verbose=0), x_train, y_train, x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T16:42:55.593751Z","iopub.execute_input":"2024-06-28T16:42:55.594184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Voting","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(random_state=42)\ncat = CatBoostClassifier(random_state=42, verbose=0)\nlgb = LGBMClassifier(random_state = 42, verbose=-1)\n\ndef voting_models(models, x_train, y_train, x_val, y_val):\n    \n    voting = VotingClassifier(estimators=models,\n                                    voting='soft')\n\n    voting.fit(x_train, y_train)\n\n    predictions_boost = voting.predict(x_val)\n\n    print(\"accuracy boost: \",  accuracy_score(y_val, predictions_boost))\n\n    # Utiliser la validation croisée pour évaluer la performance du modèle\n    scores = cross_val_score(voting, x_train, y_train, cv=5, scoring='accuracy')\n    print(\"Cross-validation accuracy scores: \", scores)\n    print(\"Mean cross-validation accuracy: \", scores.mean())\n    \n    return voting\n\nmodels = [('xgb', xgb), ('cat', cat), ('lgb', lgb)]\n\nvot = voting_models(models, x_train, y_train, x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:25:49.690302Z","iopub.execute_input":"2024-06-26T11:25:49.691413Z","iopub.status.idle":"2024-06-26T11:30:21.613050Z","shell.execute_reply.started":"2024-06-26T11:25:49.691372Z","shell.execute_reply":"2024-06-26T11:30:21.611878Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"accuracy boost:  0.7732049036777583\nCross-validation accuracy scores:  [0.77204503 0.78705441 0.76454034 0.76078799 0.76525822]\nMean cross-validation accuracy:  0.7699371966634077\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"def submission(model, test, Id):\n    pred = model.predict(test)\n\n    submission = pd.DataFrame({'id': Id,\n                               'target': pred})\n\n    # Save submission to a CSV file\n    submission.to_csv('submission.csv', index=False)\n    \nsubmission(vot, tfidf_test_vectors, id_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:30:21.614271Z","iopub.execute_input":"2024-06-26T11:30:21.614600Z","iopub.status.idle":"2024-06-26T11:30:21.748773Z","shell.execute_reply.started":"2024-06-26T11:30:21.614567Z","shell.execute_reply":"2024-06-26T11:30:21.747527Z"},"trusted":true},"execution_count":16,"outputs":[]}]}